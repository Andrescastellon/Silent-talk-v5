<!doctype html>
<html lang="es">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Traductor LSC - WebApp (Entrena en el celular)</title>
<style>
  body { font-family: system-ui, Arial; margin: 0; padding: 12px; background:#f6f7fb; color:#111 }
  header { display:flex; gap:12px; align-items:center; margin-bottom:12px }
  video, canvas { width:100%; max-height:360px; background:#000; border-radius:8px }
  .controls { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px }
  button, select { padding:10px; font-size:15px; border-radius:8px; border:1px solid #ccc; background:#fff }
  .status { margin-top:8px; padding:8px; background:#fff; border-radius:8px; border:1px solid #eee }
  .grid { display:grid; grid-template-columns: 1fr 1fr; gap:8px; margin-top:8px }
  label{font-size:14px}
  small{color:#555}
</style>
</head>
<body>
<header>
  <h2>Traductor (modo foto) — Entrena en tu Android</h2>
</header>

<main>
  <video id="video" autoplay playsinline></video>
  <canvas id="capture" style="display:none"></canvas>

  <div class="controls">
    <label for="letter">Letra/Clase:</label>
    <select id="letter"></select>

    <button id="captureSample">Capturar muestra</button>
    <button id="trainBtn">Entrenar modelo</button>
    <button id="predictBtn">Probar foto</button>
    <button id="saveModelBtn">Guardar modelo</button>
    <button id="loadModelBtn">Cargar modelo</button>
    <button id="clearDataBtn">Borrar muestras</button>
  </div>

  <div class="grid">
    <div class="status">
      <div><strong>Muestras por clase (approx.):</strong></div>
      <div id="counts">—</div>
      <small>Recomendado: ≥30 muestras por letra para empezar</small>
    </div>

    <div class="status">
      <div><strong>Estado:</strong></div>
      <div id="log">Listo. Permite cámara y comienza capturando muestras.</div>
      <small id="progress"></small>
    </div>
  </div>

  <div style="margin-top:10px">
    <small>
      Nota: este demo entrena un modelo CNN ligero en tu navegador con TensorFlow.js. La calidad depende
      de la cantidad de imágenes, la uniformidad del fondo y la iluminación.
    </small>
  </div>
</main>

<!-- TensorFlow.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0/dist/tf.min.js"></script>

<script>
/* ---------- Config UI ---------- */
const letters = Array.from(Array(26)).map((_,i)=>String.fromCharCode(65+i));
const selectLetter = document.getElementById('letter');
letters.forEach(l => {
  const opt = document.createElement('option'); opt.value = l; opt.text = l;
  selectLetter.appendChild(opt);
});

const video = document.getElementById('video');
const canvas = document.getElementById('capture');
const ctx = canvas.getContext('2d');
const countsEl = document.getElementById('counts');
const logEl = document.getElementById('log');
const progressEl = document.getElementById('progress');

let samples = {}; // { "A": [tf.Tensor image], ... }
letters.forEach(l => samples[l]=[]);

let model = null;
const IMAGE_SIZE = 64; // input size
const BATCH_SIZE = 24;
const EPOCHS = 20;

/* ---------- Camera ---------- */
async function startCamera(){
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: { ideal: "environment" } }, audio:false });
    video.srcObject = stream;
    await video.play();
    log('Cámara lista (trasera si está disponible)');
  } catch (e) {
    log('Error de cámara: ' + e.message);
  }
}
startCamera();

/* ---------- Helpers ---------- */
function log(txt){ logEl.innerText = txt; }
function updateCounts(){
  const rows = letters.map(l => `${l}: ${samples[l].length}`).join(' | ');
  countsEl.innerText = rows;
}
updateCounts();

/* ---------- Mejora 1: Normalización del recorte ---------- */
function captureImageTensor(){
  const size = IMAGE_SIZE;
  canvas.width = video.videoWidth || 640;
  canvas.height = video.videoHeight || 480;

  // Dibuja el cuadro del video
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

  // Recorte centrado (70% del lado más corto)
  const side = Math.min(canvas.width, canvas.height) * 0.7;
  const sx = (canvas.width - side) / 2;
  const sy = (canvas.height - side) / 2;

  // Prepara canvas temporal
  const tmp = document.createElement('canvas');
  tmp.width = size; tmp.height = size;
  const tctx = tmp.getContext('2d');

  // Mejora de contraste y brillo
  tctx.filter = 'contrast(140%) brightness(110%)';
  tctx.drawImage(canvas, sx, sy, side, side, 0, 0, size, size);

  const imgData = tctx.getImageData(0,0,size,size);
  let img = tf.browser.fromPixels(imgData).toFloat().div(255.0);
  return img;
}

/* ---------- Captura de muestras ---------- */
document.getElementById('captureSample').addEventListener('click', () => {
  const label = selectLetter.value;
  const t = captureImageTensor();
  samples[label].push(t);
  updateCounts();
  log(`Muestra capturada para ${label}. Total: ${samples[label].length}`);
});

/* ---------- Borrar muestras ---------- */
document.getElementById('clearDataBtn').addEventListener('click', () => {
  if (!confirm('Borrar todas las muestras almacenadas en esta sesión?')) return;
  letters.forEach(l => {
    samples[l].forEach(t => t.dispose());
    samples[l] = [];
  });
  updateCounts();
  log('Muestras borradas.');
});

/* ---------- Mejora 2: Modelo liviano ---------- */
function buildModel(numClasses){
  const m = tf.sequential();
  m.add(tf.layers.conv2d({inputShape:[IMAGE_SIZE,IMAGE_SIZE,3], filters:8, kernelSize:3, activation:'relu'}));
  m.add(tf.layers.maxPooling2d({poolSize:2}));
  m.add(tf.layers.conv2d({filters:16,kernelSize:3,activation:'relu'}));
  m.add(tf.layers.flatten());
  m.add(tf.layers.dropout({rate:0.25}));
  m.add(tf.layers.dense({units:64,activation:'relu'}));
  m.add(tf.layers.dense({units:numClasses,activation:'softmax'}));
  m.compile({optimizer: tf.train.adam(0.001), loss: 'categoricalCrossentropy', metrics:['accuracy']});
  return m;
}

/* ---------- Dataset ---------- */
function prepareDataset(){
  const xs = [];
  const ys = [];
  for (let i=0;i<letters.length;i++){
    const arr = samples[letters[i]];
    for (let j=0;j<arr.length;j++){
      xs.push(arr[j]); // tensors
      const label = new Array(letters.length).fill(0);
      label[i]=1;
      ys.push(label);
    }
  }
  if (xs.length===0) return null;
  const xStack = tf.stack(xs);
  const yTensor = tf.tensor2d(ys);
  return { x: xStack, y: yTensor };
}

/* ---------- Entrenamiento ---------- */
document.getElementById('trainBtn').addEventListener('click', async () => {
  const total = letters.reduce((s,l)=> s + samples[l].length, 0);
  if (total < 10) { log('Necesitas al menos algunas muestras (ideal ≥ 100 en total).'); return; }

  const dataset = prepareDataset();
  if (!dataset) { log('Sin datos.'); return; }

  const numClasses = letters.length;
  model = buildModel(numClasses);
  log('Entrenando modelo en el navegador...');
  progressEl.innerText = 'Entrenamiento en curso...';

  /* ---------- Mejora 3: Data augmentation ---------- */
  const augmented = dataset.x.arraySync().map(img => {
    if (Math.random() > 0.5) img.reverse(); // espejo horizontal
    return img;
  });
  dataset.x = tf.tensor4d(augmented, dataset.x.shape);

  const history = await model.fit(dataset.x, dataset.y, {
    epochs: EPOCHS,
    batchSize: Math.min(BATCH_SIZE, Math.max(8, Math.floor(dataset.x.shape[0]/4))),
    validationSplit: 0.12,
    callbacks: {
      onEpochEnd: async (epoch, logs) => {
        progressEl.innerText = `Epoch ${epoch+1}/${EPOCHS} — loss:${logs.loss.toFixed(3)} val_loss:${(logs.val_loss||0).toFixed(3)} acc:${(logs.acc||0).toFixed(3)}`;
        await tf.nextFrame();
      }
    }
  });

  progressEl.innerText = 'Entrenamiento finalizado.';
  log('Entrenamiento terminado. Prueba con "Probar foto" o guarda el modelo.');
  dataset.x.dispose();
  dataset.y.dispose();
});

/* ---------- Predicción ---------- */
document.getElementById('predictBtn').addEventListener('click', async () => {
  if (!model) {
    try {
      model = await tf.loadLayersModel('indexeddb://lsc-model');
      log('Modelo cargado desde almacenamiento local.');
    } catch(e){
      log('No hay modelo cargado. Entrena o carga uno primero.');
      return;
    }
  }
  const t = captureImageTensor();
  const input = t.expandDims(0);
  const preds = model.predict(input);
  const data = await preds.data();
  const maxIdx = data.indexOf(Math.max(...data));
  const predicted = letters[maxIdx];
  log(`Predicción: ${predicted} (conf: ${(data[maxIdx]*100).toFixed(1)}%)`);
  speak(predicted);
  t.dispose(); input.dispose(); preds.dispose();
});

/* ---------- Guardar / Cargar modelo ---------- */
document.getElementById('saveModelBtn').addEventListener('click', async () => {
  if (!model) { log('No hay modelo para guardar.'); return; }
  try {
    await model.save('indexeddb://lsc-model');
    log('Modelo guardado en almacenamiento local (IndexedDB).');
  } catch(e){
    log('Error guardando modelo: ' + e);
  }
});

document.getElementById('loadModelBtn').addEventListener('click', async () => {
  try {
    model = await tf.loadLayersModel('indexeddb://lsc-model');
    log('Modelo cargado desde IndexedDB.');
  } catch(e){
    log('No se encontró modelo guardado: ' + e);
  }
});

/* ---------- Texto a voz ---------- */
function speak(text){
  if (!('speechSynthesis' in window)) return;
  const msg = new SpeechSynthesisUtterance(text);
  msg.lang = 'es-ES';
  window.speechSynthesis.cancel();
  window.speechSynthesis.speak(msg);
}

/* ---------- Limpieza ---------- */
window.addEventListener('beforeunload', () => {
  letters.forEach(l => samples[l].forEach(t => t.dispose()));
  if (model) model.dispose();
});
</script>
</body>
</html>
